{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084918c-c17d-4e14-8659-8f10ec4e4796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1b2065-e22c-4dc2-a03f-d1f977e72206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amorvan/Documents/code_dw/llm_collection/.venv/bin/python\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import nest_asyncio\n",
    "\n",
    "# Sanity check\n",
    "print(sys.executable)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1245a232-ae02-49df-9525-03a49945aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    "    StartEvent,\n",
    "    StopEvent\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "import Stemmer\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13fcfb6-f294-4bb2-bbb4-6bea851f35e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O './paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847fbf1d-db95-473d-afd4-2a8d82893248",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.2, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741707c-d3d4-487f-991a-6959dd8901cb",
   "metadata": {},
   "source": [
    "## 1 - RAG \n",
    "\n",
    "Using the BM25 retriever system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8dd32c6-8dce-466a-a4c6-12bbb8da96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./paul_graham_essay.txt\"],\n",
    ").load_data()\n",
    "splitter = SentenceSplitter(chunk_size=256)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "retriever_top_5 = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=5,\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b33fa65-82f9-4c19-91d9-aad9fab7a660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: e03a1011-17cb-49a7-997c-ca34b209cf8a\n",
      "Text: So I'm not surprised I can't remember any programs I wrote,\n",
      "because they can't have done much. My clearest memory is of the moment\n",
      "I learned it was possible for programs not to terminate, when one of\n",
      "mine didn't. On a machine without time-sharing, this was a social as\n",
      "well as a technical error, as the data center manager's expression\n",
      "made clear....\n",
      "Score:  1.289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rez = retriever_top_5.retrieve(\"computer\")\n",
    "\n",
    "print(rez[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a7236d-e05d-43a5-9590-2e3ade02928c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was meant to be a formal model of computation, an alternative to the Turing machine. If you want to write an interpreter for a language in itself, what's the minimum set of predefined operators you need? The Lisp that John McCarthy invented, or more accurately discovered, is an answer to that question. [19]\n",
      "\n",
      "McCarthy didn't realize this Lisp could even be used to program computers till his grad student Steve Russell suggested it. Russell translated McCarthy's interpreter into IBM 704 machine language, and from that point Lisp started also to be a programming language in the ordinary sense. But its origins as a model of computation gave it a power and elegance that other languages couldn't match. It was this that attracted me in college, though I didn't understand why at the time.\n",
      "\n",
      "McCarthy's 1960 Lisp did nothing more than interpret Lisp expressions. It was missing a lot of things you'd want in a programming language. So these had to be added, and when they were, they weren't defined using McCarthy's original axiomatic approach. That wouldn't have been feasible at the time.\n"
     ]
    }
   ],
   "source": [
    "print(rez[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20291a54-7549-4fb6-9e4e-5e34225f4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever_top_5,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "response = query_engine.query(\"Who is Paul Graham.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06904a68-04af-4704-8aa6-9846ad77bba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paul Graham is a programmer and entrepreneur who worked intensively on a project called Bel, which involved writing code for an interpreter. He had to ban himself from writing essays during this time to focus on completing the project. Additionally, he is known for creating the Summer Founders Program, which attracted a significant number of applications from undergraduates and recent graduates.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02121cf-fdfc-482d-b665-cf094c68f630",
   "metadata": {},
   "source": [
    "## 2 - Exercise : \n",
    "\n",
    "Combine it with Workflows\n",
    "\n",
    "Create a workflow that : \n",
    "- Search for the best quote about the user query\n",
    "- Make a rap about it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b4327f-ab2c-4efe-a53a-6a88b62587ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContextualGrahamRapWorkflow(Workflow):\n",
    "    \n",
    "    @step\n",
    "    def do(self, ev: StartEvent) -> StopEvent:\n",
    "        return StopEvent()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a69a3533-5645-4556-a065-3027db3a5a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ContextualGrahamRapWorkflow()\n",
    "\n",
    "r = await w.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a10f6-c232-4b44-a200-c1923ed88c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616d5bf-23ee-4897-a0bb-cac5308880e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7844e41-0e22-46cc-a108-80ebdd7192a6",
   "metadata": {},
   "source": [
    "## 3 - Exercise (if time permits) \n",
    "\n",
    "Combine it with a reranker\n",
    "\n",
    "Create a workflow that : \n",
    "- Search for the best quote about the user query\n",
    "- Rerank it\n",
    "- Make a rap about it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec1be4-99d0-4ac2-871a-af7930323bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3860e0-6aa5-416c-9b8c-6767ffb3fc02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
