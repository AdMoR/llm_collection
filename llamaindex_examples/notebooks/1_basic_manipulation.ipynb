{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a34e4-b24a-4dc9-a557-63e6cdd62598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install llama_index\n",
    "!pip install llama-index-utils-workflow\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-retrievers-bm25\n",
    "!pip install llama-index-llms-azure-openai\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e0b18-72e1-4ac7-be4e-a4d7131e9721",
   "metadata": {},
   "source": [
    "## Setting up calls to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f782ae4-7e85-42c4-98a9-c57f8734b0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136c779-9f8f-427c-8312-f07fd93ea93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4edb8e8-0a5b-4624-9a8c-907fa2b32863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import nest_asyncio\n",
    "\n",
    "# Sanity check\n",
    "print(sys.executable)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "OPENAI_API_MODEL_NAME = \"gpt-4o\"\n",
    "OPENAI_DEV_KEY = \"***************\"\n",
    "OPENAI_API_TYPE = \"azure\"\n",
    "OPENAI_API_VERSION = \"2023-03-15-preview\"\n",
    "OPENAI_API_BASE = \"https://*******.openai.azure.com/\"\n",
    "OPENAI_MODEL_EMBEDDING = \"text-embedding-ada-002\"\n",
    "\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "\"\"\"\n",
    "OPENAI_API_MODEL_NAME = os.environ[\"OPENAI_API_MODEL_NAME\"]\n",
    "OPENAI_DEV_KEY = os.environ[\"OPENAI_DEV_KEY\"]\n",
    "OPENAI_API_TYPE = os.environ[\"OPENAI_API_TYPE\"]\n",
    "OPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\n",
    "OPENAI_API_BASE = os.environ[\"OPENAI_API_BASE\"]\n",
    "OPENAI_MODEL_EMBEDDING = os.environ[\"OPENAI_MODEL_EMBEDDING\"]\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2cfa1-d14b-4746-83c7-82dc6ee8143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "# Using Azure OpenAI\n",
    "temperature = 0\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "        engine=OPENAI_API_MODEL_NAME,\n",
    "        model=OPENAI_API_MODEL_NAME,\n",
    "        api_key=OPENAI_DEV_KEY,\n",
    "        azure_endpoint=OPENAI_API_BASE,\n",
    "        api_version=OPENAI_API_VERSION,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98aff9-194c-4baf-b82d-361a65b49ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful to not load\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", api_key=os.environ[\"OPENAI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa0921-e08b-40b1-9688-0c989cb2e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(\"Paul Graham is \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab28cf-101d-4d48-a6c3-46a5f68cba7e",
   "metadata": {},
   "source": [
    "## Your first Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d0994-8f38-4eb1-b1ef-3fa90538fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    "StartEvent, StopEvent\n",
    ")\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def do_it(self, ev: StartEvent) -> StopEvent:\n",
    "        return StopEvent(result=\"coucou\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a7895-a9bb-4fe0-b3e3-69db4d7f8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(MyWorkflow, filename=\"basic_workflow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fb2c9-1554-4f93-a0bd-8ad862163f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = MyWorkflow()\n",
    "\n",
    "r = await w.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc7115f-5cc5-4101-adbf-af057ec8692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3d0bc-1754-4900-9a9d-4849b755d444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98053d27-4fb3-4baf-8c02-e238b189e5eb",
   "metadata": {},
   "source": [
    "## Exercise : \n",
    "\n",
    "### Make a workflow that answers the query of a user\n",
    "\n",
    "User : What is an Ipod\n",
    "\n",
    "```python\n",
    "w = MyCustomWorkflow()\n",
    "result = await w.run()\n",
    "print(result)\n",
    "```\n",
    "\"An ipod is ....\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226899f6-8590-4471-976d-d7bf4a034c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyCustomWorkflow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def answer(self, ev: StartEvent) -> StopEvent:\n",
    "        pass\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4f1b1-0d3a-457a-b50d-3f146dc3956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is an Ipod\"\n",
    "\n",
    "w = MyCustomWorkflow()\n",
    "result = await w.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b26e4-4fd9-4c2f-b997-b020df0318cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d315b-7b1e-43a5-9e2d-bc94bc6e6746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d35571-55c8-4969-9545-97a0d5f7a963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fce5af8-2271-4262-988c-fd58ed68ba34",
   "metadata": {},
   "source": [
    "### A more complex worflow\n",
    "\n",
    "Answer in two steps : \n",
    "- Find the answer of the question\n",
    "- Transform it into a poem\n",
    "\n",
    "Use a prompt template for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5f7d6-4556-4a54-829d-609fc4831015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"{value}\"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "prompt = qa_template.format(value=\"\")\n",
    "\n",
    "\n",
    "\n",
    "class MyNewWorkflow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def step_a(self, ev: StartEvent) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "    @step\n",
    "    async def step_b(self, ev: None) -> StopEvent:\n",
    "        pass\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6dbd81-dddf-47f5-a1b8-5aad987b8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is an Ipod\"\n",
    "\n",
    "w = MyNewWorkflow()\n",
    "result = await w.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a93dd-1760-4c96-9791-3b4d9c3732b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc01af1-45f4-44db-b75e-4480c226a6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
