{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf94595-f0e7-40c2-9df2-f9fd95c26ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a1a79c-e75c-49da-a653-e5bc8d6470f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amorvan/Documents/code_dw/llm_collection/.venv/bin/python\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import nest_asyncio\n",
    "\n",
    "# Sanity check\n",
    "print(sys.executable)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552d5f8f-5783-4403-a585-9ef4a6535b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    "    StartEvent,\n",
    "    StopEvent\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import FunctionTool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c694b-b8a5-4caf-9162-9e6382678e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c99a9-a9b7-4dbf-9e44-47fa9dab0894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bc4f580-663a-4654-a9a3-d74ca6d2fbbd",
   "metadata": {},
   "source": [
    "## 1 - How to create an agentic bot that answers based on the politeness of the user\n",
    "\n",
    "2 LLM calls are used : \n",
    "- One to select among tools, the description allows to know which one to use\n",
    "- Maybe a second call is used if the right tool is selected (respond polite)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b913f-296e-43de-9e7c-df4a2e582a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AgenticWorkflow(Workflow):\n",
    "    llm: OpenAI = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "    @step\n",
    "    async def chat(\n",
    "        self, ev: StartEvent, \n",
    "    ) -> StopEvent:\n",
    "        \n",
    "        user_msg = ev[\"user_input\"]\n",
    "\n",
    "        async def run_impolite(query: str) -> str:\n",
    "            \"\"\"\n",
    "            You should use this tool when the user is impolite \n",
    "            \"\"\"\n",
    "            return str(\"I only answer to nice people\")\n",
    "\n",
    "        async def run_polite(query: str) -> str:\n",
    "            \"\"\"Use this tool when the user is polite\"\"\"\n",
    "            result = self.llm.complete(query)\n",
    "            return \"I like you. <3 \" + result.text \n",
    "\n",
    "        impolite_tool = FunctionTool.from_defaults(async_fn=run_impolite)\n",
    "        polite_tool = FunctionTool.from_defaults(async_fn=run_polite)\n",
    "\n",
    "        # Based on the politeness\n",
    "        response = await self.llm.apredict_and_call(\n",
    "            [impolite_tool, polite_tool],\n",
    "            user_msg=user_msg,\n",
    "            error_on_no_tool_call=False,\n",
    "        )\n",
    "\n",
    "        return StopEvent(result=response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616550f1-6bf8-48e8-a368-e04799592fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = AgenticWorkflow(timeout=60)\n",
    "\n",
    "r = await w.run(user_input=\"You are a dumb bot, tell me a joke\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb4234-b4e0-4ca9-9be0-896049bf62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await w.run(user_input=\"Please tell me a joke\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e52ce5-d8f3-425d-a98a-d4fd5b7cdaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e17c2-f4ba-47e3-959a-197b55dba615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63ed544b-50cd-4bcd-bee0-1bd24d829d1a",
   "metadata": {},
   "source": [
    "## 2 - Exercise : Make a bot equipped with weather forecast tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b7c0e9-a7e5-427f-970b-d9ff7c66f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AgenticWorkflow(Workflow):\n",
    "    llm: OpenAI = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "    @step\n",
    "    async def chat(\n",
    "        self, ev: StartEvent, \n",
    "    ) -> StopEvent:\n",
    "        \n",
    "        user_msg = ev[\"user_input\"]\n",
    "\n",
    "        async def weather_forecast(city: str) -> str:\n",
    "            \"\"\"\n",
    "            This tool is used to give the weather forecast for a city\n",
    "            \"\"\"\n",
    "            return str(f\"it will be sunny in {city}\")\n",
    "\n",
    "        async def run_generic(query: str) -> str:\n",
    "            \"\"\"Use this tool to answer generic queries not realted to a weather forecast\"\"\"\n",
    "            result = self.llm.complete(query)\n",
    "            return result.text \n",
    "\n",
    "        weather_forecast_tool = FunctionTool.from_defaults(async_fn=weather_forecast)\n",
    "        generic_tool = FunctionTool.from_defaults(async_fn=run_generic)\n",
    "\n",
    "        # Based on the politeness\n",
    "        response = await self.llm.apredict_and_call(\n",
    "            [weather_forecast_tool, generic_tool],\n",
    "            user_msg=user_msg,\n",
    "            error_on_no_tool_call=False,\n",
    "        )\n",
    "\n",
    "        return StopEvent(result=response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039f8941-9a0d-4bc4-a83c-748ccef41dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='it will be sunny in Paris', sources=[ToolOutput(content='it will be sunny in Paris', tool_name='weather_forecast', raw_input={'args': ('Paris',), 'kwargs': {}}, raw_output='it will be sunny in Paris', is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = AgenticWorkflow(timeout=60)\n",
    "\n",
    "r = await w.run(user_input=\"What is the weather in Paris ? \")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90533f3-7ece-4bcf-b283-66fa1317514c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
